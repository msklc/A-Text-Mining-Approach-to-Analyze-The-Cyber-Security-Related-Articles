{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Text Mining Approach to Analyze The Cyber Security Related Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART-3: Text Mining\n",
    "\n",
    "__Feature Engineering with NLP techniques__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing of Required Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from autocorrect import Speller\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading Data__\n",
    "\n",
    "As mentioned Part-2 that pandas parquet options doesn't support timedelta type. So we need to use __fastparquet__ option, to keep the timedelta type format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>AwardTitle</th>\n",
       "      <th>AwardEffectiveDate</th>\n",
       "      <th>AwardExpirationDate</th>\n",
       "      <th>AwardAmount</th>\n",
       "      <th>ProgramOfficer</th>\n",
       "      <th>Institution_Name</th>\n",
       "      <th>Institution_StateName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Award_Duration</th>\n",
       "      <th>AwardAmount_Million</th>\n",
       "      <th>Abstract_Lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>Collaborative Research Testing Affect Control ...</td>\n",
       "      <td>2001-08-15</td>\n",
       "      <td>2004-07-31</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>Patricia White</td>\n",
       "      <td>University of Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1081 days</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>Federal Cyber Service Initiative</td>\n",
       "      <td>2001-06-01</td>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>149995.0</td>\n",
       "      <td>Timothy V. Fossum</td>\n",
       "      <td>University of Tulsa</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>2001</td>\n",
       "      <td>2220 days</td>\n",
       "      <td>0.15</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                         AwardTitle  \\\n",
       "0  0110599  Collaborative Research Testing Affect Control ...   \n",
       "1  0112426                   Federal Cyber Service Initiative   \n",
       "\n",
       "  AwardEffectiveDate AwardExpirationDate  AwardAmount     ProgramOfficer  \\\n",
       "0         2001-08-15          2004-07-31     300000.0     Patricia White   \n",
       "1         2001-06-01          2007-06-30     149995.0  Timothy V. Fossum   \n",
       "\n",
       "        Institution_Name Institution_StateName  \\\n",
       "0  University of Arizona               Arizona   \n",
       "1    University of Tulsa              Oklahoma   \n",
       "\n",
       "                                            Abstract  Year Award_Duration  \\\n",
       "0  The investigators will conduct a series of exp...  2001      1081 days   \n",
       "1  This program produces a cadre of computer scie...  2001      2220 days   \n",
       "\n",
       "   AwardAmount_Million  Abstract_Lenght  \n",
       "0                 0.30             2194  \n",
       "1                 0.15              765  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum=pd.read_parquet('df_sum_parquet.gzip',engine='fastparquet')\n",
    "df_investigator_cyber=pd.read_parquet('df_investigator_cyber_parque.gzip',engine='fastparquet')\n",
    "df_sum.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-) Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Replacing Of Contraction Words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>AwardTitle</th>\n",
       "      <th>AwardEffectiveDate</th>\n",
       "      <th>AwardExpirationDate</th>\n",
       "      <th>AwardAmount</th>\n",
       "      <th>ProgramOfficer</th>\n",
       "      <th>Institution_Name</th>\n",
       "      <th>Institution_StateName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Award_Duration</th>\n",
       "      <th>AwardAmount_Million</th>\n",
       "      <th>Abstract_Lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>Collaborative Research Testing Affect Control ...</td>\n",
       "      <td>2001-08-15</td>\n",
       "      <td>2004-07-31</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>Patricia White</td>\n",
       "      <td>University of Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1081 days</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>Federal Cyber Service Initiative</td>\n",
       "      <td>2001-06-01</td>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>149995.0</td>\n",
       "      <td>Timothy V. Fossum</td>\n",
       "      <td>University of Tulsa</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>2001</td>\n",
       "      <td>2220 days</td>\n",
       "      <td>0.15</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                         AwardTitle  \\\n",
       "0  0110599  Collaborative Research Testing Affect Control ...   \n",
       "1  0112426                   Federal Cyber Service Initiative   \n",
       "\n",
       "  AwardEffectiveDate AwardExpirationDate  AwardAmount     ProgramOfficer  \\\n",
       "0         2001-08-15          2004-07-31     300000.0     Patricia White   \n",
       "1         2001-06-01          2007-06-30     149995.0  Timothy V. Fossum   \n",
       "\n",
       "        Institution_Name Institution_StateName  \\\n",
       "0  University of Arizona               Arizona   \n",
       "1    University of Tulsa              Oklahoma   \n",
       "\n",
       "                                            Abstract  Year Award_Duration  \\\n",
       "0  The investigators will conduct a series of exp...  2001      1081 days   \n",
       "1  This program produces a cadre of computer scie...  2001      2220 days   \n",
       "\n",
       "   AwardAmount_Million  Abstract_Lenght  \n",
       "0                 0.30             2194  \n",
       "1                 0.15              765  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace contraction words to alternative forms before tokenization\n",
    "replacement_patterns = {\n",
    "    r\"won\\'t\": \"will not\",\n",
    "    r\"can\\'t\": \"cannot\",\n",
    "    r\"i\\'m\": \"i am\",\n",
    "    r\"ain\\'t\": \"is not\",\n",
    "    r\"(\\w+)\\'ll\": \"\\g<1> will\",\n",
    "    r\"(\\w+)n\\'t\": \"\\g<1> not\",\n",
    "    r\"(\\w+)\\'ve\": \"\\g<1> have\",\n",
    "    r\"(\\w+)\\'s\": \"\\g<1> is\",\n",
    "    r\"(\\w+)\\'re\": \"\\g<1> are\",\n",
    "    r\"(\\w+)\\'d\": \"\\g<1> would\",\n",
    "    r\"&\": \"and\",\n",
    "    r\"<br/>\":\" \"}\n",
    "\n",
    "df_sum['Abstract']=df_sum['Abstract'].replace(replacement_patterns, regex=True)\n",
    "df_sum.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Merge All Values in Abstract Columns to A Single String__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total character number of all Abstract columns is: 17055166\n"
     ]
    }
   ],
   "source": [
    "abstract_list=df_sum['Abstract'].tolist()\n",
    "abstract_sum=' '.join(abstract_list)\n",
    "print('Total character number of all Abstract columns is: {}'.format(len(abstract_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenization and LowerCase__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lowercase words: 2669124\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "wrd_list = nltk.word_tokenize(abstract_sum)\n",
    "\n",
    "#lowercase\n",
    "wrd_list=[w.lower() for w in wrd_list]\n",
    "print('Total lowercase words: {}'.format(len(wrd_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove Numbers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words consist of only alphabets: 2318555\n"
     ]
    }
   ],
   "source": [
    "#remove numbers\n",
    "wrd_list_alpha=[w for w in wrd_list if w.isalpha()]\n",
    "print('Total words consist of only alphabets: {}'.format(len(wrd_list_alpha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove Written Form of Numbers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words consist of only alphabets: 2312021\n"
     ]
    }
   ],
   "source": [
    "#Create numbers in written form for ignoring\n",
    "num_written= {1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', \\\n",
    "             6: 'six', 7: 'seven', 8: 'eight', 9: 'nine', 10: 'ten', \\\n",
    "            11: 'eleven', 12: 'twelve', 13: 'thirteen', 14: 'fourteen', \\\n",
    "            15: 'fifteen', 16: 'sixteen', 17: 'seventeen', 18: 'eighteen', \\\n",
    "            19: 'nineteen', 20: 'twenty', 30: 'thirty', 40: 'forty', \\\n",
    "            50: 'fifty', 60: 'sixty', 70: 'seventy', 80: 'eighty', \\\n",
    "            90: 'ninety', 0: 'zero'}\n",
    "\n",
    "num_list=list(np.arange(0,100))\n",
    "\n",
    "num2words_list=[]\n",
    "\n",
    "def num2words(n):\n",
    "    try:\n",
    "        return num_written[n]\n",
    "    except:\n",
    "        try:\n",
    "            return num2words(n-n%10) + num2words(n%10)\n",
    "        except:\n",
    "            return 'None'\n",
    "\n",
    "        \n",
    "for num in num_list:\n",
    "    num2words_list.append(num2words(num))\n",
    "    \n",
    "wrd_list_rm_written_form = [w for w in wrd_list_alpha if w not in num2words_list]\n",
    "print('Total words consist of only alphabets: {}'.format(len(wrd_list_rm_written_form)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove Stopwords__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after removing stopwords: 1414326\n"
     ]
    }
   ],
   "source": [
    "#define stopwords list\n",
    "stopwords_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#extend the stopwords list\n",
    "stopwords_list.extend(['cannot','many','much','also','well','better','via'])\n",
    "\n",
    "wrd_list_rm_stopwords = [w for w in wrd_list_rm_written_form if w not in stopwords_list]\n",
    "print('Total words after removing stopwords: {}'.format(len(wrd_list_rm_stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove 1-Length Words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after removing 1-len words: 1413212\n"
     ]
    }
   ],
   "source": [
    "#remove 1-len words\n",
    "wrd_list_one_len = [w for w in wrd_list_rm_stopwords if len(w)>=2]\n",
    "\n",
    "print('Total words after removing 1-len words: {}'.format(len(wrd_list_one_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Counting The Words Frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words before lemmatization: 32965\n"
     ]
    }
   ],
   "source": [
    "freq_dict_before=nltk.FreqDist(wrd_list_one_len)\n",
    "print('Total unique words before lemmatization: {}'.format(len(freq_dict_before)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 frequency words:\n",
      "[('research', 19624), ('project', 16513), ('data', 12358), ('systems', 9147), ('students', 9052), ('new', 8583), ('science', 7363), ('security', 5927), ('development', 5220), ('system', 5189), ('information', 5066), ('program', 4725), ('support', 4665), ('design', 4638), ('education', 4430), ('engineering', 4424), ('university', 4368), ('network', 4344), ('using', 4343), ('develop', 4295), ('community', 4291), ('learning', 4287), ('use', 4228), ('software', 4049), ('provide', 3953), ('materials', 3869), ('tools', 3679), ('technology', 3587), ('cyberinfrastructure', 3458), ('applications', 3353), ('researchers', 3284), ('broader', 3278), ('computing', 3248), ('work', 3210), ('cybersecurity', 3193), ('infrastructure', 3101), ('control', 3082), ('nsf', 3076), ('computational', 3055), ('computer', 3046), ('scientific', 3045), ('analysis', 3042), ('methods', 3036), ('high', 2969), ('including', 2938), ('used', 2913), ('award', 2896), ('national', 2846), ('impact', 2801), ('models', 2761)]\n"
     ]
    }
   ],
   "source": [
    "print('First 50 frequency words:\\n{}'.format(freq_dict_before.most_common(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words after lemmatization: 29316\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "wrd_list_lemmas = [lemmatizer.lemmatize(w) for w in wrd_list_one_len]\n",
    "\n",
    "#Recounting the Words Frequency\n",
    "freq_dict=nltk.FreqDist(wrd_list_lemmas)\n",
    "\n",
    "print('Total unique words after lemmatization: {}'.format(len(freq_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 frequency words:\n",
      "[('research', 19634), ('project', 18206), ('system', 14336), ('data', 12358), ('student', 10612), ('new', 8583), ('science', 8549), ('network', 6920), ('program', 6462), ('security', 5927), ('technology', 5892), ('community', 5716), ('support', 5579), ('impact', 5509), ('development', 5476), ('university', 5085), ('information', 5066), ('design', 5035), ('model', 4935), ('application', 4789), ('material', 4609), ('tool', 4439), ('education', 4433), ('engineering', 4424), ('using', 4343), ('develop', 4295), ('learning', 4287), ('use', 4228), ('software', 4051), ('provide', 3953), ('approach', 3580), ('process', 3561), ('method', 3535), ('infrastructure', 3523), ('cyberinfrastructure', 3458), ('researcher', 3441), ('computer', 3424), ('resource', 3423), ('study', 3381), ('activity', 3380), ('analysis', 3368), ('work', 3324), ('broader', 3278), ('control', 3252), ('computing', 3248), ('cybersecurity', 3193), ('nsf', 3078), ('computational', 3055), ('scientific', 3045), ('workshop', 3015)]\n"
     ]
    }
   ],
   "source": [
    "print('First 50 frequency words:\\n{}'.format(freq_dict.most_common(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Saving The Result__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research</td>\n",
       "      <td>19634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>18206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>system</td>\n",
       "      <td>14336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words  Count\n",
       "0  research  19634\n",
       "1   project  18206\n",
       "2    system  14336"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_FreqDist=pd.DataFrame({'Words':list(freq_dict.keys()),'Count':list(freq_dict.values())})\n",
    "df_FreqDist.sort_values(by=['Count'],ascending=False,inplace=True)\n",
    "df_FreqDist=df_FreqDist.reset_index(drop=True)\n",
    "df_FreqDist.to_csv('FreqDist.csv')\n",
    "df_FreqDist.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B-) Clustering and Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract\n",
       "0  0110599  The investigators will conduct a series of exp...\n",
       "1  0112426  This program produces a cadre of computer scie..."
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new DataFrame\n",
    "df_sum2=df_sum[['AwardID','Abstract']][:30]\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Replacing Of Contraction Words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract\n",
       "0  0110599  The investigators will conduct a series of exp...\n",
       "1  0112426  This program produces a cadre of computer scie..."
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_patterns = {\n",
    "    r\"won\\'t\": \"will not\",\n",
    "    r\"can\\'t\": \"cannot\",\n",
    "    r\"i\\'m\": \"i am\",\n",
    "    r\"ain\\'t\": \"is not\",\n",
    "    r\"(\\w+)\\'ll\": \"\\g<1> will\",\n",
    "    r\"(\\w+)n\\'t\": \"\\g<1> not\",\n",
    "    r\"(\\w+)\\'ve\": \"\\g<1> have\",\n",
    "    r\"(\\w+)\\'s\": \"\\g<1> is\",\n",
    "    r\"(\\w+)\\'re\": \"\\g<1> are\",\n",
    "    r\"(\\w+)\\'d\": \"\\g<1> would\",\n",
    "    r\"&\": \"and\",\n",
    "    r\"<br/>\":\" \"}\n",
    "\n",
    "df_sum2['Abstract'].replace(replacement_patterns, regex=True, inplace=True)\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing Unneccesery Words__\n",
    "\n",
    "We use __NER__ (Named Entity Recognition) function of __spacy__ library to detect named entities (people, places, organizations, dates, times etc.) from the text. After analyzing this words, we understood that they are unnecessary for clustering.\n",
    "\n",
    "An example of visualization of NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This program produces a cadre of computer scientists with strong specializations in information assurance and a commitment to federal service. \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " cohorts of students complete a \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two-year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " program that integrates intense information assurance studies with research and outreach. Students also spend \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one summer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " as interns in federal agencies. Upon completion of degrees at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the end of two years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " the students then enter the federal cyber service. The program features an emphasis on collaborative research and outreach to the community. The program components train students in information assurance theory and practice while providing an environment that fosters teamwork, strengthens motivation, and builds a sense of professionalism and commitment to service.  </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define the nlp object\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc=nlp(str(df_sum2['Abstract'][1]))\n",
    "\n",
    "displacy.render(doc, style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique NER words: 32\n",
      "Total words after removing NER: 54787\n",
      "Total words after removing NER: 53845\n"
     ]
    }
   ],
   "source": [
    "#create a list for NER\n",
    "NER_list=[]\n",
    "\n",
    "#Find\n",
    "for n in range(df_sum2.shape[0]):\n",
    "    doc=nlp(str(df_sum2['Abstract'][n]))\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        NER_list.append(ent.text)\n",
    "\n",
    "NER_list=list(set(NER_list)) #for unique elements in list\n",
    "print('Total unique NER words: {}'.format(len(NER_list)))\n",
    "\n",
    "print('Total words after removing NER: {}'.format(sum(df_sum2['Abstract'].str.len())))\n",
    "\n",
    "for NER in NER_list:\n",
    "    df_sum2['Abstract']=df_sum2['Abstract'].str.replace(NER,'')\n",
    "print('Total words after removing NER: {}'.format(sum(df_sum2['Abstract'].str.len())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenization and LowerCase__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 8733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[the, investigators, will, conduct, a, series,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[this, program, produces, a, cadre, of, comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \n",
       "0  [the, investigators, will, conduct, a, series,...  \n",
       "1  [this, program, produces, a, cadre, of, comput...  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization and lowercase\n",
    "df_sum2['Abstract_Tokens'] = df_sum2['Abstract'].str.lower().apply(nltk.word_tokenize)\n",
    "print('Total words: {}'.format(sum(df_sum2['Abstract_Tokens'].str.len())))\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove Numbers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words consist of only alphabets: 7525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[the, investigators, will, conduct, a, series,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[this, program, produces, a, cadre, of, comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \n",
       "0  [the, investigators, will, conduct, a, series,...  \n",
       "1  [this, program, produces, a, cadre, of, comput...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove numbers\n",
    "for n in range(df_sum2.shape[0]):\n",
    "    df_sum2['Abstract_Tokens'][n]=[w for w in (abstract for abstract in df_sum2['Abstract_Tokens'][n]) if w.isalpha()]\n",
    "print('Total words consist of only alphabets: {}'.format(sum(df_sum2['Abstract_Tokens'].str.len())))\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove Stopwords__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after removing stopwords: 4300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[investigators, conduct, series, experiments, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[program, produces, cadre, computer, scientist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \n",
       "0  [investigators, conduct, series, experiments, ...  \n",
       "1  [program, produces, cadre, computer, scientist...  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define stopwords list\n",
    "stopwords_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#extend the stopwords list\n",
    "stopwords_list.extend(['cannot','many','much','also','well','better','via'])\n",
    "\n",
    "#extend the stopwords list with unneccessary words\n",
    "stopwords_list.extend(['abstract','many','much','also','well','better','via'])\n",
    "\n",
    "\n",
    "for n in range(df_sum2.shape[0]):\n",
    "    df_sum2['Abstract_Tokens'][n]=[w for w in (abstract for abstract in df_sum2['Abstract_Tokens'][n]) if w not in stopwords_list]\n",
    "print('Total words after removing stopwords: {}'.format(sum(df_sum2['Abstract_Tokens'].str.len())))\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after lemmatization: 4300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[investigator, conduct, series, experiment, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[program, produce, cadre, computer, scientist,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \n",
       "0  [investigator, conduct, series, experiment, fo...  \n",
       "1  [program, produce, cadre, computer, scientist,...  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for n in range(df_sum2.shape[0]):\n",
    "    df_sum2['Abstract_Tokens'][n]=[lemmatizer.lemmatize(w) for w in (abstract for abstract in df_sum2['Abstract_Tokens'][n])]\n",
    "\n",
    "print('Total words after lemmatization: {}'.format(sum(df_sum2['Abstract_Tokens'].str.len())))\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove 1-Length Words__\n",
    "\n",
    "Punctuation\n",
    "Punctuation are the unnecessary symbols that are in our corpus documents,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after removing 1-len words: 4279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[investigator, conduct, series, experiment, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[program, produce, cadre, computer, scientist,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \n",
       "0  [investigator, conduct, series, experiment, fo...  \n",
       "1  [program, produce, cadre, computer, scientist,...  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove 1-len words\n",
    "for n in range(df_sum2.shape[0]):\n",
    "    df_sum2['Abstract_Tokens'][n]=[w for w in (abstract for abstract in df_sum2['Abstract_Tokens'][n]) if len(w)>=2]\n",
    "print('Total words after removing 1-len words: {}'.format(sum(df_sum2['Abstract_Tokens'].str.len())))\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract2 uniqi!!!\n",
    "#Recounting the Words Frequency\n",
    "freq_dict=nltk.FreqDist(wrd_list_lemmas)\n",
    "\n",
    "print('Total unique words after lemmatization: {}'.format(len(freq_dict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert Tokens To String Again__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "      <th>Abstract2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[investigator, conduct, series, experiment, fo...</td>\n",
       "      <td>investigator conduct series experiment focus c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[program, produce, cadre, computer, scientist,...</td>\n",
       "      <td>program produce cadre computer scientist stron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \\\n",
       "0  [investigator, conduct, series, experiment, fo...   \n",
       "1  [program, produce, cadre, computer, scientist,...   \n",
       "\n",
       "                                           Abstract2  \n",
       "0  investigator conduct series experiment focus c...  \n",
       "1  program produce cadre computer scientist stron...  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum2['Abstract2']=df_sum2['Abstract']\n",
    "for n in range(df_sum2.shape[0]):\n",
    "    df_sum2['Abstract2'][n]=' '.join([str(item) for item in df_sum2['Abstract_Tokens'][n]])\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "X = v.fit_transform(df_sum2['Abstract2'])\n",
    "X.toarray()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'academic',\n",
       " 'accelerate',\n",
       " 'acceptable',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accompanying',\n",
       " 'accountability',\n",
       " 'accuracy']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create New DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>academic</th>\n",
       "      <th>accelerate</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>access</th>\n",
       "      <th>accessible</th>\n",
       "      <th>accompanying</th>\n",
       "      <th>accountability</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>workforce</th>\n",
       "      <th>working</th>\n",
       "      <th>workplace</th>\n",
       "      <th>workshop</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>would</th>\n",
       "      <th>yet</th>\n",
       "      <th>yit</th>\n",
       "      <th>youth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1418 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  academic  accelerate  acceptable  access  accessible  \\\n",
       "0      0.0   0.0       0.0         0.0         0.0     0.0         0.0   \n",
       "1      0.0   0.0       0.0         0.0         0.0     0.0         0.0   \n",
       "\n",
       "   accompanying  accountability  accuracy  ...  workforce  working  workplace  \\\n",
       "0           0.0             0.0       0.0  ...        0.0      0.0        0.0   \n",
       "1           0.0             0.0       0.0  ...        0.0      0.0        0.0   \n",
       "\n",
       "   workshop  world  worldwide  would  yet  yit  youth  \n",
       "0       0.0    0.0        0.0    0.0  0.0  0.0    0.0  \n",
       "1       0.0    0.0        0.0    0.0  0.0  0.0    0.0  \n",
       "\n",
       "[2 rows x 1418 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=X.toarray(), columns=v.get_feature_names())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  5,  5,  5,  4,  3,  5,  0,  4,  3,  0, 10,  6,  9, 11, 11,  5,\n",
       "        4,  6,  8,  5,  1,  6,  7,  7,  2,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=12)\n",
    "#Run the clustering algorithm\n",
    "model = k_means.fit(df)\n",
    "model\n",
    "#Generate cluster predictions and store in y_hat\n",
    "y_hat = k_means.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for each value of k, we can initialise k_means and use inertia to identify the sum of squared distances of samples to the nearest cluster centre\n",
    "sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k)\n",
    "    model = k_means.fit(df)\n",
    "    sum_of_squared_distances.append(k_means.inertia_)\n",
    "    \n",
    "    \n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('sum_of_squared_distances')\n",
    "plt.title('Elbow method for optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=3)\n",
    "#Run the clustering algorithm\n",
    "model = k_means.fit(df)\n",
    "model\n",
    "#Generate cluster predictions and store in y_hat\n",
    "y_hat = k_means.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "      <th>Abstract2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110599</td>\n",
       "      <td>The investigators will conduct a series of exp...</td>\n",
       "      <td>[investigator, conduct, series, experiment, fo...</td>\n",
       "      <td>investigator conduct series experiment focus c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112426</td>\n",
       "      <td>This program produces a cadre of computer scie...</td>\n",
       "      <td>[program, produce, cadre, computer, scientist,...</td>\n",
       "      <td>program produce cadre computer scientist stron...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AwardID                                           Abstract  \\\n",
       "0  0110599  The investigators will conduct a series of exp...   \n",
       "1  0112426  This program produces a cadre of computer scie...   \n",
       "\n",
       "                                     Abstract_Tokens  \\\n",
       "0  [investigator, conduct, series, experiment, fo...   \n",
       "1  [program, produce, cadre, computer, scientist,...   \n",
       "\n",
       "                                           Abstract2  Label  \n",
       "0  investigator conduct series experiment focus c...      2  \n",
       "1  program produce cadre computer scientist stron...      2  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=list(y_hat)\n",
    "df_sum2['Label']=labels\n",
    "df_sum2.to_csv('df_sum_clustering.csv')\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Tokens</th>\n",
       "      <th>Abstract2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0225421</td>\n",
       "      <td>The science erk () project seeks to bring l...</td>\n",
       "      <td>[science, erk, project, seek, bring, informati...</td>\n",
       "      <td>science erk project seek bring information man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0225473</td>\n",
       "      <td>The science erk () project seeks to bring l...</td>\n",
       "      <td>[science, erk, project, seek, bring, informati...</td>\n",
       "      <td>science erk project seek bring information man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0225495</td>\n",
       "      <td>Meers  The science erk () project seeks to br...</td>\n",
       "      <td>[meers, science, erk, project, seek, bring, in...</td>\n",
       "      <td>meers science erk project seek bring informati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0225508</td>\n",
       "      <td>The science erk () project seeks to bring l...</td>\n",
       "      <td>[science, erk, project, seek, bring, informati...</td>\n",
       "      <td>science erk project seek bring information man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0225543</td>\n",
       "      <td>Arrowsmith  The science erk () project seeks ...</td>\n",
       "      <td>[arrowsmith, science, erk, project, seek, brin...</td>\n",
       "      <td>arrowsmith science erk project seek bring info...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AwardID                                           Abstract  \\\n",
       "25  0225421     The science erk () project seeks to bring l...   \n",
       "26  0225473     The science erk () project seeks to bring l...   \n",
       "27  0225495   Meers  The science erk () project seeks to br...   \n",
       "28  0225508     The science erk () project seeks to bring l...   \n",
       "29  0225543   Arrowsmith  The science erk () project seeks ...   \n",
       "\n",
       "                                      Abstract_Tokens  \\\n",
       "25  [science, erk, project, seek, bring, informati...   \n",
       "26  [science, erk, project, seek, bring, informati...   \n",
       "27  [meers, science, erk, project, seek, bring, in...   \n",
       "28  [science, erk, project, seek, bring, informati...   \n",
       "29  [arrowsmith, science, erk, project, seek, brin...   \n",
       "\n",
       "                                            Abstract2  Label  \n",
       "25  science erk project seek bring information man...      0  \n",
       "26  science erk project seek bring information man...      0  \n",
       "27  meers science erk project seek bring informati...      0  \n",
       "28  science erk project seek bring information man...      0  \n",
       "29  arrowsmith science erk project seek bring info...      0  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum2[df_sum2['Label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make array from number of occurrences\n",
    "occ = np.asarray(X.sum(axis=0)).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability</td>\n",
       "      <td>0.290547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablamowicz</td>\n",
       "      <td>0.030533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>able</td>\n",
       "      <td>0.055422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abstract</td>\n",
       "      <td>0.074508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>academic</td>\n",
       "      <td>0.204240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>yamacrew</td>\n",
       "      <td>0.103869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>year</td>\n",
       "      <td>0.197243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>yet</td>\n",
       "      <td>0.055461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>york</td>\n",
       "      <td>0.091565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>youth</td>\n",
       "      <td>0.037318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1649 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            term  occurrences\n",
       "0        ability     0.290547\n",
       "1     ablamowicz     0.030533\n",
       "2           able     0.055422\n",
       "3       abstract     0.074508\n",
       "4       academic     0.204240\n",
       "...          ...          ...\n",
       "1644    yamacrew     0.103869\n",
       "1645        year     0.197243\n",
       "1646         yet     0.055461\n",
       "1647        york     0.091565\n",
       "1648       youth     0.037318\n",
       "\n",
       "[1649 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowListFrame = pd.DataFrame({'term': v.get_feature_names(), 'occurrences': occ})\n",
    "bowListFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-64-9445b50fd6c6>\", line 1, in <module>\n",
      "    import texthero as hero\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\texthero\\__init__.py\", line 12, in <module>\n",
      "    from . import visualization\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\texthero\\visualization.py\", line 9, in <module>\n",
      "    from nltk import NLTKWordTokenizer\n",
      "ImportError: cannot import name 'NLTKWordTokenizer' from 'nltk' (C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-64-9445b50fd6c6>\", line 1, in <module>\n",
      "    import texthero as hero\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\texthero\\__init__.py\", line 12, in <module>\n",
      "    from . import visualization\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\texthero\\visualization.py\", line 9, in <module>\n",
      "    from nltk import NLTKWordTokenizer\n",
      "ImportError: cannot import name 'NLTKWordTokenizer' from 'nltk' (C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Users\\WIN\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLTKWordTokenizer' from 'nltk' (C:\\Users\\WIN\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import texthero as hero\n",
    "df_sum2['tfidf'] = hero.tfidf(df_sum2['Abstract2'])\n",
    "df_sum2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TEST__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfs = tfidf.fit_transform(df_sum2['Abstract'].values)\n",
    "tfs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_tf_idf = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model_tf_idf.fit(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NearestNeighbors' object has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4e8753e91880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_tf_idf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NearestNeighbors' object has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "model_tf_idf.transforms(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
